\section{Related Work}
En ~\cite{compositeRetrival} se sugieren diferentes algoritmos para hallar una solución al problema. Produce and Choose es uno de ellos y en el que se encuetra basado este artículo. En el artículo original se mencionan otras dos alternativas más para la solución de este tipo de problemas, una basada en técnicas de clustering y otra en programación lineal. Es por las los resultados obtenidos de las ejecuciones con PAC que se decidió utilizarlo para realizar los cambios propuestos.\\
La estructura del algoritmo permite optimizar y agregar mejoras para obtener mejores resultados. Se implementó otra heurística con un enfoque diferente como fue una de tipo golosa pero por los resultados obtenidos y el tiempo de ejecución en este trabajo nos enfocaremos unicamente en el algoritmo PAC y las mejoras que se realizaron en él al añadirle búsquedas locales.\\
En la etapa de producción de los bundles puede ocurrir que los mismos no sean los óptimos ya que no se trata de algoritmos exactos. Una vez finalizada la etapa de producción y a diferencia de las soluciones anteriores, se intenta mejorar aquellos bundles que por un tema de ordenamiento y elección de lo items no resultaron siendo mejores. El tema de utilizar el resto de los items del universo que no fueron escogidos para formar parte de la solución final en pos de mejorar la solución, modificando los bundles ya generados, es clave para las mejoras propuestas.
\subsection{Data Model}\label{body-data-model}
El modelo de datos de la instancia de \textit{articulo italianos} contiene las entidades: artículos, autores, venues, affiliations y topics. Los artículos están etiquetados con un topic profile que representa el porcentaje de cada tópico encontrado en el artículo, así por ejemplo el paper \textit{paper1} esta catalogado con los siguientes tópicos: \textit{50\% topic1, 50\% topic2}. A partir del topic profile de cada uno de los artículos se pudo definir la noción de similitud entre los artículos.\\
Como también es interesante hacer consultas sobre los autores, se necesitaba que éstos también tengan un perfil el cuál no se encontraba en la base de datos. Con el objetivo de no depender de ninguna otra fuente se utilizaron los perfiles de los artículos para lograr el objetivo y de de esta manera definir su similitud. En orden de lograr un perfil, para cada autor se tomaron todos los papers en los cuales figura como autor y se sumaron los porcentajes de cada uno de los tópicos y luego normalizaron los valores para que tomen valore válidos (entre 0 y 1). Si bien no es cierto que el perfil del autor es que aquí se calcula ya que solo contiene información acotado, pero a medida que la base de datos se complete esta información será cada vez más precisa.\\
Utilizando la misma técnica anteriormente descripta se puede obtener el perfil del resto de los objetos (Universidad, venue).\\
Para generar los resultados de \textit{composite} se tiene un conjunto de objetos bibliográficos $I$ que son unívocamente identificado y contienen un conjunto de atributos y una función de similitud entre los objetos $ s: I \times I \rightarrow [0;1]$. En este trabajo la función de similitud se definió a partir del coseno del vector del topic profile.\\
\subsection{Problem Statement}
Formalmente el problema consiste en dado un conjunto de items $ I=\left\{i_1 \ldots i_n\right\} $, una función de similitud $ s(u,v) $ para cada par $ (u,v) \in IxI $ un atributo complementario $\alpha$, una función budget, un presupesto y un entero k se debe hallar $ S=\left\{S_1 \ldots S_k\right\} $ que maximice la función:
\begin{equation} \label{des:eq-fnObj}
\sum_{1 \leq i \leq k}{\sum_{u,v \in S_i}{\gamma s(u,v)}} + \sum_{1 \leq i \leq j \leq k}{(1-\gamma) (1-\max_{u \in S_i, v \in S_j}{s(u,v)})}
\end{equation}
Cada elemento $s_i \in S$ es válido si y sólo si satisface las reglas:
\begin{itemize}
	\item \textbf{Complementaridad:} dado el atributo $\alpha$ de los objetos, $\forall u,v \in s_i, u.\alpha \neq v.\alpha$
	\item \textbf{Presupuesto:} dada la función de costo $f$ y el presupuesto $\beta$, entonces $\forall s_i \in S, f(s_i) \leq \beta$, donde $f(s_i)$ es la suma de costos de los elementos pertenecientes al bundle.
\end{itemize}
La formula ~\ref{des:eq-fnObj} es una típica función objetivo de un problema de clustering, donde la calidad del clustering es una combinación entre la calidad de cada cluster (intra-cluster) y de la separación entre clusters (inter-cluster). A través del parámetro $\gamma$ el usuario puede definir el balance entre intra e inter de una solución. Si El usuario prioriza una solución de bundles cohesivos sobre la diversidad el  valor de $\gamma$ será cercano a uno y si lo que prioriza es la diversidad el valor estará cerca de cero.\\
\section{Algorithm}
\subsection{Produce and Choose}
Como se mencionó anteriormente el alogoritmo con el que se obtuvo los mejores resultados fue PAC. PAC consiste en dos partes: primero en generar un conjunto de bundles válidos y luego seleccionar los k mejores que formarán parte de la solución. Para la parte de generación de bundles se plantearon los métodos de clusterización \texttt{Efficient C-HAC} y \texttt{k-BOBO}.\\
\texttt{Efficient C-HAC} (Efficient Constrained hierarchical agglomerative clustering) es un adaptación de un algoritmo de clusterización jerárquico aglomerativo. En este algoritmo inicialmente cada objeto pertenece a un cluster unitario y luego sucesivamente se unen un par de clusters para generar un cluster válido. El algoritmo finaliza cuando no existan el par de bundles $S_1$ y $S_2$ tal que el bundle $S_1 \cup S_2 $ sea válido, esto es porque el costo de $S_1 \cup S_2$ supera el presupuesto o porque algún elemento de $S_1$ tiene un atributo igual que un elemento de $S_2$.\\
El método \texttt{BOBO-k} (Bundles One-By-One), está inspirado en k-means, consiste en generar $k$ cluster del conjunto de $n$ ítems. El algoritmo comienza con todos los items del conjunto $I$ como posibles pivots $P$. Se selecciona un pivote de $P$ y con los elementos de $I$ se genera un bundle válido alrededor de este, en caso que el bundle generado sea suficientemente bueno se agrega al conjunto de bundles candidatos y los ítems del bundle se eliminan de $I$. La generación de bundles continúa hasta que se cumpla el criterio de parada que es la generación de $k$ bundles.\\
Al finalizar la producción de bundles comienza la etapa de selección de bundles en la cual se deben seleccionar los $k$ bundles para la solución. El problema de seleccionar los bundles que maximizan la función objetivo se traduce en encontrar en el grafo completo G con peso en los nodos y vértices (el peso de los nodos representa la calidad de los bundles y el peso de las aristas es la distancia entre los nodos) el k-subgrafo de mayor peso (considerando los nodos y vértices). Para ello se implemento un algoritmo goloso, por el cual se selecciona iterativamente del conjunto de bundles aquel que máximiza la funcion objetivo.\\
\subsection{Tabu Search}
Con el objetivo de encontrar una solución más óptima de la obtenida por PAC, se realizo dos implementaciones de la metaheuristica tabu search. La \texttt{Inter-Bundle} explora soluciones vecinas intercambiando bundles que pertenecen a la solución con los otros bundles producidos en la etapa de producción pero no son parte de la solución. La otra implementación es \texttt{Intra-Bundle} que explora las soluciones vecinas reemplazando el item de menor similitud del centroide del bundle menos cohesivo de la solución por algún ítem que no pertenezca a la solución con mayor similitud al centroide. Para ambos casos el criterio de parada es por la cantidad de soluciones vecinas visitadas.
