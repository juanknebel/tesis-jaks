En la literatura pueden encontrarse diversos trabajos que abordan la noción de \emph{recuperación de la información} con semánticas diferentes. La mayoría de las investigaciones tienen en común que ofrecen al usuario resultados en forma de conjuntos o paquetes de elementos de interés. Las diferencias radican en la construcción de los paquetes, las definiciones de similitud entre sus elementos y la diversidad de los resultados. A continuación se mencionan dos publicaciones relacionados con la recuperación de la información.

En \cite{BasuRoy:2010:CEC:1807167.1807258} se plantea recuperar la información complementaria a un ``ítem central'' (\textit{composite item}). La información obtenida es un conjunto de ``paquetes satélites'', que son conjuntos de ítems de diferente tipo al ``ítem central'' pero a su vez compatibles entre ellos. Un escenario de aplicación podría ser por ejemplo, un comprador de un teléfono celular (ítem central) que dispone de cierto presupuesto al que se le ofrecen paquetes que incluyen otros ítems relacionados como funda, tapa, parlantes, cargador, etc. El objetivo implica identificar todos los paquetes satélites válidos y maximales a partir de un ítem central. Un paquete maximal y válido es un conjunto de elementos que respetan un presupuesto, son compatibles con el ítem central y no son un subconjunto de otro paquete válido. Para la construcción de los paquetes satélites se utiliza un algoritmo que elige aleatoriamente ítems que sean compatibles con el ítem central y cumpla las restricciones de presupuesto y validez, luego implementan algoritmos \emph{greedy} basados en técnicas de clustering para lograr encontrar el mejor cubrimiento de los paquetes encontrados.

En \cite{Xie:2010:BOB:1864708.1864739} se sugiere que el resultado de un sistema de recomendación debería ser un conjunto de paquetes y no una clásica lista ordenada por los ítems más relevantes. Los autores consideran que muchas aplicaciones se beneficiarían al utilizar este criterio. Por ejemplo, podría aplicarse para un turista que está interesado en sugerencias de lugares o puntos de interés para su viaje. Suponiendo que dispone de un determinado tiempo y un presupuesto máximo, y que maneja una noción de compatibilidad entre ítems: visitar hasta tres museos, que el recorrido sea menor a cierta distancia, etc. Si cada atracción tiene asociado un ranking, la idea es ofrecer los mejores $k$ paquetes de atracciones de tal manera que en cada paquete se respete el presupuesto y la compatibilidad entre los ítems. El algoritmo para generar el resultado que propone el artículo se obtiene a partir de una variación del algoritmo utilizado para resolver el problema de la mochila \cite{DBLP:conf/coco/Karp72} y \cite{Gossett:2009:DMP:1717238} para valores enteros. 

Si bien el objetivo de ambos trabajos es entregar conjuntos de ítems relacionados que generan un valor agregado al usuario de la aplicación, ninguno tiene en cuenta la diversidad entre los paquetes sugeridos, formalizando la recuperación de la información como un problema de {\em clustering} que sólo considera la compatibilidad entre ítems. Como alternativa, en \cite{journals/tkde/Amer-YahiaBCFMZ14} se propone conjugar, en el conjunto de paquetes sugeridos al usuario, las nociones de diversidad, similitud y compatibilidad dependiendo de la necesidad y elección del mismo.

Los algoritmos que se presentarán en esta tesis en los capítulos siguientes se basan en las ideas propuestas en \cite{journals/tkde/Amer-YahiaBCFMZ14}, por tal motivo serán aquí descriptos con un nivel de detalle mayor en comparación a los descriptos en los trabajos citados previamente.

\begin{itemize}

	\item {\em Produce and Choose} (PAC): Utiliza un esquema de dos fases, donde en la primera se producen paquetes válidos y en la segunda se eligen $k$ entre ellos. Para la producción de paquetes se implementaron dos algoritmos de {\em clustering}, uno basado en {\em clustering jerárquico aglomerativo} (\textit{C-HAC}) y el segundo en {\em k-means clustering} denominado \textit{BOBO} (Bundles One-By-One), debido a que en cada paso se elige un elemento y a partir de él se construye un paquete (o bundle). Para la segunda fase se adaptaron heurísticas de la literatura para el problema {\em Maximum Edge Subgraph}.

En C-HAC inicialmente cada ítem forma un {\em cluster} unitario y luego sucesivamente en cada iteración se elige un par de {\em clusters} para ser unidos y generar un nuevo {\em cluster} válido, es decir que cumpla las restricciones de complementariedad y no supere el presupuesto $\beta$. El algoritmo finaliza cuando no existe un par de {\em clusters} que pueda ser unido en uno nuevo válido o se alcance una condición de parada (por ejemplo, un cierto número de {\em clusters} generados). Como criterio de elección de los {\em clusters} a unir se busca maximizar la función $Score$, que es la suma de las similitudes de todos los ítems del par candidato. Este criterio de unión sólo presta atención a la similitud intra-paquete, lo que hace que, cuando se busca una alta diversidad, el conjunto de paquetes generados de esa forma no sea necesariamente bueno.

{\em BOBO-c} está inspirado en el conocido método k-means \cite{forgy65}. En términos simples, consiste en generar $c*k$ clusters de un conjunto $I$ de $n$ ítems. El algoritmo comienza con todos los ítems del conjunto $I$ como posibles pivotes en un conjunto $P$. Se selecciona un pivote, $p \in P$ y con los elementos de $I$ se genera un {\em cluster} válido ``alrededor" de éste. En caso que el {\em cluster} generado sea suficientemente bueno (su valor \texttt{intra} supera un parámetro $\mu$) se agrega al conjunto de paquetes candidatos y los ítems del {\em cluster} se eliminan de $P$. La generación de paquetes continúa hasta que se cumpla el criterio de parada elegido; en este caso, la generación de $c*k$ paquetes. La estrategia \textit{BOBO-c} puede dejar ítems excluidos de los paquetes generados. A raíz de la limitación del algoritmo, los autores desarrollaron una variante a la que llamaron \textit{BOBO-E'} (por Exhaustivo), el cual logra que todos los ítems pertenezcan a algún paquete. La desventaja radica en que su complejidad aumenta en comparación a la implementación inicial, impidiendo debido a sus tiempos de ejecución su aplicación para grandes bases de datos. Entonces para lograr el objetivo de la nueva propuesta exhaustiva se modificó el criterio de parada original por uno nuevo, es decir la nueva condición de parada es hasta que el conjunto de pivotes $P$ sea vacío.

Anteriormente se mencionó que los autores dividieron el problema en dos etapas. En primer lugar la producción descripta previamente y la fase de selección de paquetes la cual se detallará a continuación. Luego de la producción de paquetes se deben seleccionar los mejores $k$ que formarán la solución (selección simple). El problema de seleccionar los paquetes que maximizan la función objetivo se traduce en encontrar en un grafo completo $G$, el subgrafo de $k$ vértices de mayor peso (considerando los pesos de los vértices y aristas). Los vértices representan los paquetes (con pesos asociados dados por sus valores \texttt{intra}) y cuyas aristas tienen como peso los valores \texttt{inter}. Con el problema debidamente identificado, los autores del trabajo \cite{journals/tkde/Amer-YahiaBCFMZ14} implementaron un algoritmo goloso. La implementación para la resolución del problema consiste en seleccionar iterativamente un paquete del conjunto que maximice la función objetivo evaluada en la solución temporal que se tiene hasta el momento. Se menciona que en las primeras iteraciones de la selección el valor \texttt{inter} (o inter-paquetes) es despreciable con respecto a la suma de los valores \texttt{intra} (o intra-paquete). Como consecuencia concluyen que cuando se intenta privilegiar la diversidad de paquetes en la solución final (valores bajos de $\gamma$), esta selección no representa una buena estrategia.

Como resultado de la forma en que las soluciones son generadas en las heurísticas del tipo {\em Produce and choose} (construir una cantidad suficiente de paquetes y luego seleccionar un subconjunto de éstos), es de esperar que las soluciones generadas se enfoquen más en valorar la parte intra-paquete que la inter-paquete.

\item {\em Cluster and Pick} (CAP): Sigue un esquema de dos fases al igual que las propuestas anteriores. La diferencia radica en que la primera etapa es la encargada de generar $k$ {\em clusters} con alto grado de cohesión interno y una buena separación externa. Esto puede ser logrado con cualquier algoritmo conocido de ``clustering''. Se debe tener en cuenta que los {\em clusters} aquí obtenidos no necesariamente cumplen con las condiciones impuestas en la definición del problema, por ende puede ocurrir con alta probabilidad que no sean paquetes válidos. Luego comienza la segunda fase encargada de tomar cada uno de los {\em clusters} previamente construidos y seleccionar el mejor paquete válido para cada uno de ellos. Todos los paquetes obtenidos en la segunda etapa son los que formarán parte de la solución.

\item Método IP: Se presenta un modelo de programación lineal entera que resuelve el problema de forma exacta. Los tiempos de ejecución de este enfoque lo hacen prohibitivo para grandes instancias.

\end{itemize}

Los algoritmos propuestos son evaluados en \cite{journals/tkde/Amer-YahiaBCFMZ14} sobre dos bases de datos reales. Una de ellas se refiere a atracciones turísticas en 10 ciudades europeas, donde por cada una de estas instancias se consideran 20 atracciones. El segundo conjunto de datos se compone de opiniones sobre restaurantes en 149 ciudades provenientes del sitio de Yahoo! Local. El número de restaurantes en cada ciudad varía de 300 a 2000. De la experimentación presentada, se concluye que las mejores soluciones se obtienen con los algoritmos que primero agrupan los ítems en paquetes válidos y luego seleccionan paquetes dentro de este agrupamiento (PAC).

En el próximo capítulo se presentarán mejoras a los algoritmos y estrategias aquí detallados. Las modificaciones realizadas tendrán como foco principal otorgarle mayor peso a los valores \texttt{inter} de los paquetes, en la producción y selección de los mismos. La introducción de los cambios será con el fin de obtener soluciones más diversas en comparación a las actuales. Para realizarlo se presentará una nueva función de similitud entre los paquetes al momento de su producción. 

Como nuevas alternativas se propondrán: un nuevo algoritmo completamente diferente a todos los mencionados y una estrategia para mejorar las soluciones una vez construidas sin importar cuál fue el método que originó la solución.
